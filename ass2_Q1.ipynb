{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce591778",
   "metadata": {},
   "source": [
    "### Question 1 - [40 Points] Scoring and Term-Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6894b872",
   "metadata": {},
   "source": [
    "##### Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a5489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict , Counter\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer as ps\n",
    "from nltk.stem import WordNetLemmatizer as wl\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd416a46",
   "metadata": {},
   "source": [
    "#####  Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02723b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "def preprocess(sent):\n",
    "    result = sent.lower() # Converting to lower case\n",
    "    result = re.sub(r'[^\\w\\s]','',result) # remove punctuation\n",
    "    result = \" \".join(result.split()) #remove blank spaces\n",
    "    result = word_tokenize(result) # Word tokenizer\n",
    "    result = [word for word in result if word not in stopword] # removing stopwords\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c81357",
   "metadata": {},
   "source": [
    "##### Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791209ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Files 1133\n"
     ]
    }
   ],
   "source": [
    "mypath = 'dataset/'\n",
    "files = [f for f in listdir(mypath) if isfile(join(mypath, f))] # all file names present in the folder\n",
    "print(\"Total Files\" , len(files))\n",
    "fileSet = set(files)   \n",
    "stopword = set(stopwords.words()) # set of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "debd98bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filesp = {}  # key:file name , value file content\n",
    "for i in files:\n",
    "    f =open('dataset/'+i, \"r\", encoding='cp850')\n",
    "    filesp[i] = set(preprocess(f.read()))  # preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8628d8",
   "metadata": {},
   "source": [
    "### Jaccard Coefficient [20 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e65b2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jac_coeff(query):\n",
    "    query = set(preprocess(query))\n",
    "    ans = []\n",
    "    for i in filesp:\n",
    "        uni = filesp[i].union(query) # union\n",
    "        inter = filesp[i].intersection(query) # intesection\n",
    "        ans.append( (i , len(inter) / len(uni) , inter , len(uni))) # Jaccard Coefficient\n",
    "    ans.sort(key = lambda x:-x[1])\n",
    "    print(\"TOP 5 DOCUMENTS ARE:-\")\n",
    "    print(\"File name   jaccard coefficient   words_present    total words\")\n",
    "    for i in range(5): # TOP 5 relevant queries with their jaccard Coefficient\n",
    "        print(ans[i])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35fa082b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of Queries:  4\n",
      "cold water\n",
      "TOP 5 DOCUMENTS ARE:-\n",
      "File name   jaccard coefficient   words_present    total words\n",
      "('pasta001.sal', 0.022222222222222223, {'water', 'cold'}, 90)\n",
      "('japice.bev', 0.0196078431372549, {'water'}, 51)\n",
      "('orgfrost.bev', 0.01818181818181818, {'cold'}, 55)\n",
      "('montoys.txt', 0.017857142857142856, {'water'}, 56)\n",
      "('antimead.bev', 0.017543859649122806, {'water', 'cold'}, 114)\n",
      "\n",
      "\n",
      "\n",
      "bad boy\n",
      "TOP 5 DOCUMENTS ARE:-\n",
      "File name   jaccard coefficient   words_present    total words\n",
      "('buffwing.pol', 0.01818181818181818, {'bad'}, 55)\n",
      "('childrenbooks.txt', 0.015384615384615385, {'boy', 'bad'}, 130)\n",
      "('normal.boy', 0.012195121951219513, {'boy'}, 82)\n",
      "('normalboy.txt', 0.012195121951219513, {'boy'}, 82)\n",
      "('forsooth.hum', 0.011904761904761904, {'boy'}, 84)\n",
      "\n",
      "\n",
      "\n",
      "king kong\n",
      "TOP 5 DOCUMENTS ARE:-\n",
      "File name   jaccard coefficient   words_present    total words\n",
      "('jrrt.riddle', 0.010638297872340425, {'king'}, 94)\n",
      "('smokers.txt', 0.008, {'king'}, 125)\n",
      "('yogisays.txt', 0.006369426751592357, {'king'}, 157)\n",
      "('adcopy.hum', 0.006060606060606061, {'king'}, 165)\n",
      "('fuck!.txt', 0.005847953216374269, {'king'}, 171)\n",
      "\n",
      "\n",
      "\n",
      "dark humor\n",
      "TOP 5 DOCUMENTS ARE:-\n",
      "File name   jaccard coefficient   words_present    total words\n",
      "('curiousgeorgie.txt', 0.011494252873563218, {'dark', 'humor'}, 174)\n",
      "('ookpik.hum', 0.010416666666666666, {'dark'}, 96)\n",
      "('blaster.hum', 0.01, {'dark'}, 100)\n",
      "('renorthr.txt', 0.009708737864077669, {'humor'}, 103)\n",
      "('dandwine.bev', 0.009615384615384616, {'dark'}, 104)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in range(int(input(\"Enter number of Queries:  \"))):\n",
    "    jac_coeff(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0f423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "419fe097",
   "metadata": {},
   "source": [
    "### TF-IDF Matrix [20 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038864dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e068b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = Counter() # IDF matrix\n",
    "files_p = {}  \n",
    "for i in files:\n",
    "    f =open('dataset/'+i, \"r\", encoding='cp850')\n",
    "    files_p[i] = preprocess(f.read())  # preprocessing the data \n",
    "    for i in set(files_p[i] ):\n",
    "        idf[i] += 1  \n",
    "for i in idf:\n",
    "    idf[i] = math.log(len(files)/(idf[i] + 1 ))  # IDF(word)=log(total no. of documents/document frequency(word)+1)\n",
    "vocab = set(idf.keys()) # vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "663a9ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_tfidf():  # Binary\n",
    "    tf = {}\n",
    "    tf_idf = defaultdict(Counter)\n",
    "    for i in files:\n",
    "        f = files_p[i]\n",
    "        tf[i] = Counter(list(set(f)))\n",
    "        for j in tf[i]:\n",
    "            tf_idf[i][j] = tf[i][j] * idf[j]\n",
    "    return tf_idf\n",
    "b_tfidf = binary_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f422b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_tfidf(): # Raw count\n",
    "    tf = {}\n",
    "    tf_idf = defaultdict(Counter)\n",
    "    for i in files:\n",
    "        f = files_p[i]\n",
    "        tf[i] = Counter(f)\n",
    "        for j in tf[i]:\n",
    "            tf_idf[i][j] = tf[i][j] * idf[j]\n",
    "    return tf_idf\n",
    "r_tfidf = raw_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be075aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq_tf_idf():# Term frequency\n",
    "    tf = {}\n",
    "    tf_idf = defaultdict(Counter)\n",
    "    for i in files:\n",
    "        f = files_p[i]\n",
    "        tf[i] = Counter(f)\n",
    "        totalwords = len(f)\n",
    "        for j in tf[i]:\n",
    "            tf_idf[i][j] = tf[i][j] * idf[j] / totalwords\n",
    "    return tf_idf\n",
    "tf_tfidf = term_freq_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c883db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_tf_idf(): # Log normalization\n",
    "    tf = {}\n",
    "    tf_idf = defaultdict(Counter)\n",
    "    for i in files:\n",
    "        f = files_p[i]\n",
    "        tf[i] = Counter(f)\n",
    "        for j in tf[i]:\n",
    "            tf_idf[i][j] = math.log( tf[i][j] + 1 ) * idf[j]\n",
    "    return tf_idf\n",
    "ln_tfidf =  ln_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d606cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dn_freq_tf_idf(): # Double normalization\n",
    "    tf = {}\n",
    "    tf_idf = defaultdict(lambda: defaultdict(lambda: 0.5))\n",
    "    for i in files:\n",
    "        f = files_p[i]\n",
    "        tf[i] = Counter(f)\n",
    "        maxwords = max(list(tf[i].values()))\n",
    "        for j in tf[i]:\n",
    "            tf_idf[i][j] +=  0.5 * (tf[i][j] * idf[j] / maxwords)\n",
    "    return tf_idf\n",
    "dn_tfidf = dn_freq_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66e40b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tf_idf , query):  # Function for evaluating the query\n",
    "    ans = []\n",
    "    query = {i for i in query if query[i] == 1}\n",
    "    for i in tf_idf:\n",
    "        score = 0\n",
    "        for j in query:\n",
    "            score += tf_idf[i][j] \n",
    "        ans.append((i , score))\n",
    "    ans.sort(key = lambda x:-x[1])\n",
    "    for i in range(5):\n",
    "        print(ans[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94ee3d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter number of Queries:  3\n",
      "Enter Query: king kong\n",
      "For  Binary Weighting Scheme\n",
      "('cast.lis', 6.18671432591565)\n",
      "('classicm.hum', 6.18671432591565)\n",
      "('commutin.jok', 6.18671432591565)\n",
      "('consp.txt', 6.18671432591565)\n",
      "('drunk.txt', 6.18671432591565)\n",
      "\n",
      "\n",
      "For  Raw count Weighting Scheme\n",
      "('epi_merm.txt', 163.8946738319551)\n",
      "('grail.txt', 60.61857799264093)\n",
      "('blackadd', 26.941590218951525)\n",
      "('pun.txt', 20.206192664213642)\n",
      "('quest.hum', 17.961060145967682)\n",
      "\n",
      "\n",
      "For  Term frequency Weighting Scheme\n",
      "('epi_merm.txt', 0.0507413850872926)\n",
      "('hotel.txt', 0.03675134552605772)\n",
      "('yogisays.txt', 0.025368729019728364)\n",
      "('pun.txt', 0.025226208070179328)\n",
      "('epitaph', 0.025057282569709377)\n",
      "\n",
      "\n",
      "For  Log normalization Weighting Scheme\n",
      "('epi_merm.txt', 9.663196501400012)\n",
      "('grail.txt', 7.481240703240202)\n",
      "('insult.lst', 7.442684760707859)\n",
      "('classicm.hum', 6.796800384929975)\n",
      "('episimp2.txt', 6.3454977102363115)\n",
      "\n",
      "\n",
      "For Double normalization Weighting Scheme\n",
      "('hotel.txt', 2.313860602556564)\n",
      "('pun.txt', 1.841924694342235)\n",
      "('blooprs1.asc', 1.673539755473788)\n",
      "('yogisays.txt', 1.56128312956149)\n",
      "('epi_merm.txt', 1.4792241924910967)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Enter Query: dark humor\n",
      "For  Binary Weighting Scheme\n",
      "('aids.txt', 4.6437562116179985)\n",
      "('bitnet.txt', 4.6437562116179985)\n",
      "('c0dez.txt', 4.6437562116179985)\n",
      "('collected_quotes.txt', 4.6437562116179985)\n",
      "('consp.txt', 4.6437562116179985)\n",
      "\n",
      "\n",
      "For  Raw count Weighting Scheme\n",
      "('dark.suc', 149.46431487016736)\n",
      "('candy.txt', 55.357153655617545)\n",
      "('insult.lst', 42.44048446930678)\n",
      "('insults1.txt', 33.214292193370525)\n",
      "('coffee.faq', 22.142861462247016)\n",
      "\n",
      "\n",
      "For  Term frequency Weighting Scheme\n",
      "('dark.suc', 0.25637103751315155)\n",
      "('fegg!int.txt', 0.027302612257860945)\n",
      "('cheapfar.hum', 0.0257927903818502)\n",
      "('renorthr.txt', 0.02544107051300679)\n",
      "('exylic.txt', 0.020826619133038952)\n",
      "\n",
      "\n",
      "For  Log normalization Weighting Scheme\n",
      "('dark.suc', 8.131447816265817)\n",
      "('jason.fun', 7.5723143552353065)\n",
      "('mlverb.hum', 6.531242218211336)\n",
      "('dead5.txt', 6.437613050581496)\n",
      "('candy.txt', 6.3365252443369595)\n",
      "\n",
      "\n",
      "For Double normalization Weighting Scheme\n",
      "('dark.suc', 1.9226192275936258)\n",
      "('exylic.txt', 1.9226192275936256)\n",
      "('renorthr.txt', 1.6996294391076867)\n",
      "('radexposed.txt', 1.5088214102601358)\n",
      "('cheapfar.hum', 1.4664196260717912)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Enter Query: cold water\n",
      "For  Binary Weighting Scheme\n",
      "('a-team', 3.204846662809358)\n",
      "('aids.txt', 3.204846662809358)\n",
      "('all_grai', 3.204846662809358)\n",
      "('aniherb.txt', 3.204846662809358)\n",
      "('antimead.bev', 3.204846662809358)\n",
      "\n",
      "\n",
      "For  Raw count Weighting Scheme\n",
      "('candy.txt', 944.6386440574819)\n",
      "('penndtch', 199.4174262353557)\n",
      "('vegan.rcp', 160.4054476113923)\n",
      "('all_grai', 122.5011063279311)\n",
      "('bread.rcp', 114.11226903870244)\n",
      "\n",
      "\n",
      "For  Term frequency Weighting Scheme\n",
      "('pasta001.sal', 0.06411954153256026)\n",
      "('antimead.bev', 0.05709330636191865)\n",
      "('hierarch.txt', 0.049831353243576335)\n",
      "('gack!.txt', 0.044183799875971014)\n",
      "('strattma.txt', 0.04047370981004979)\n",
      "\n",
      "\n",
      "For  Log normalization Weighting Scheme\n",
      "('candy.txt', 17.557947714687394)\n",
      "('penndtch', 12.423997585499489)\n",
      "('vegan.rcp', 10.989282697735103)\n",
      "('coffee.faq', 9.402241140509222)\n",
      "('byfb.txt', 9.232740457698238)\n",
      "\n",
      "\n",
      "For Double normalization Weighting Scheme\n",
      "('foodtips', 2.0241671263184555)\n",
      "('beginn.ers', 1.7616692437464192)\n",
      "('beer.txt', 1.7180558921573226)\n",
      "('meat2.txt', 1.7122131209429923)\n",
      "('antimead.bev', 1.7089085539938234)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in range(int(input(\"Enter number of Queries:  \"))):\n",
    "    query = set(preprocess(input(\"Enter Query: \")))\n",
    "    query_vector = {}\n",
    "    for i in vocab:\n",
    "        if i in query:\n",
    "            query_vector[i] = 1 \n",
    "        else:\n",
    "            query_vector[i] = 0 \n",
    "    \n",
    "    print(\"For  Binary Weighting Scheme\")\n",
    "    evaluate(b_tfidf , query_vector)\n",
    "    print(\"\\n\\nFor  Raw count Weighting Scheme\")\n",
    "    evaluate(r_tfidf , query_vector)\n",
    "    print(\"\\n\\nFor  Term frequency Weighting Scheme\")\n",
    "    evaluate(tf_tfidf , query_vector)\n",
    "    print(\"\\n\\nFor  Log normalization Weighting Scheme\")\n",
    "    evaluate(ln_tfidf , query_vector)\n",
    "    print(\"\\n\\nFor Double normalization Weighting Scheme\")\n",
    "    evaluate(dn_tfidf , query_vector)\n",
    "    print(\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833ad100",
   "metadata": {},
   "source": [
    "#### Note: Answers for binary weighting scheme will vary a lot because most of the documents have same tf_idf score.\n",
    "#### In all other queries, answers may differ a little bit because of the preprocessing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a1354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47020de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa345bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6feac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e84ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b6c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
